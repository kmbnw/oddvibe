/*
 * Copyright 2016-2017 Krysta M Bouzek
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <iostream>
#include <iomanip>
#include <random>
#include <cmath>
#include <vector>
#include <algorithm>
#include <iterator>
#include <functional>
#include "train_data.h"
#include "seq_sampler.h"
#include "cached_sampler.h"
#include "ecdf_sampler.h"
#include "booster.h"
#include "booster_test.h"

#include <cppunit/extensions/TestFactoryRegistry.h>
#include <cppunit/ui/text/TestRunner.h>
#include <cppunit/extensions/HelperMacros.h>

CPPUNIT_TEST_SUITE_REGISTRATION(oddvibe::BoosterTest);

namespace oddvibe {

    void BoosterTest::setUp() {
    }

    void BoosterTest::tearDown() {
    }

    void BoosterTest::test_add_counts_cached() {
        const std::vector<float> pmf { 0.4f, 0.25f, 0.15f, 0.20f };
        EmpiricalSampler sampler(time(0), pmf);

        const size_t nrows = pmf.size();
        const size_t max_iter = 1000;
        std::vector<size_t> expected_counts(nrows, 0);

        CachedSampler cache(sampler);

        for (size_t i = 0; i < max_iter; ++i) {
            for(size_t k = 0; k < nrows; ++k) {
                expected_counts[cache.next_sample()]++;
            }
        }

        std::vector<size_t> counts(nrows, 0);
        for (size_t i = 0; i < max_iter; ++i) {
            add_counts(cache, counts);
        }

        for (size_t k = 0; k < nrows; ++k) {
            CPPUNIT_ASSERT(expected_counts[k] == counts[k]);
        }

        // expect sequential calls to give us the same values from the CachedSampler
        std::vector<size_t> counts2(nrows, 0);
        for (size_t i = 0; i < max_iter; ++i) {
            add_counts(cache, counts2);
        }

        for (size_t k = 0; k < nrows; ++k) {
            CPPUNIT_ASSERT(expected_counts[k] == counts2[k]);
        }
    }

    void BoosterTest::test_add_counts() {
        const std::vector<float> pmf { 0.4f, 0.25f, 0.15f, 0.20f };
        EmpiricalSampler sampler(time(0), pmf);

        const size_t nrows = pmf.size();
        const size_t max_iter = 100000;
        const size_t sample_sz = max_iter * nrows;

        std::vector<size_t> counts(nrows, 0);
        for (size_t i = 0; i < max_iter; ++i) {
            add_counts(sampler, counts);
        }

        // should have about the same distribution as the initial PMF
        for (size_t k = 0; k != counts.size(); ++k) {
            // note: this could technically fail, but it is quite unlikely
            CPPUNIT_ASSERT(counts[k] > 0);

            float pct = (float) (((double) counts[k]) / sample_sz);
            //std::cout << "X == " << k << ": " << pct << std::endl;
            CPPUNIT_ASSERT_DOUBLES_EQUAL(pmf[k], pct, 1e-2);
        }
    }

    void BoosterTest::test_fit() {
        // mixture distribution generated by two separate linear equations
        // but with the same noise

        const size_t seed = time(0); //1480561820L; //1480455481L; //time(0);
        const size_t nrows = 50;
        const size_t nfeatures = 2;
        const float intercept = 0.75f;
        const float beta_1 = 2.0f;
        const float beta_2 = 5.8f;

        std::cout << "Seed: " << seed << std::endl;
        std::normal_distribution<float> noise_dist(0.0, 1.0);
        std::mt19937 rand_engine(seed);

        auto gen = std::bind(noise_dist, rand_engine);
        std::vector<float> xs_noise(nrows * 2);
        std::generate(std::begin(xs_noise), std::end(xs_noise), gen);

        // nominally small x values are associated with small Y values
        // but we want to have this find the outliers
        // so mix it up
        std::normal_distribution<float> feature_x1_dist(5.0, 1.0);
        std::normal_distribution<float> feature_x2_dist(4000.3, 90.0);

        std::vector<float> xs(nrows * nfeatures, 0);
        std::vector<float> ys(nrows, 0);

        size_t threshold = (size_t) (0.7 * nrows);

        for (size_t k = 0; k != xs.size(); ++k) {
            if (k < (threshold * 2)) {
                xs[k] = feature_x1_dist(rand_engine);
            } else {
                xs[k] = feature_x2_dist(rand_engine);
            }
        }

        for (size_t k = 0, row_idx = 0; k != ys.size(); ++k, row_idx += nfeatures) {
            ys[k] = intercept + beta_1 * xs[row_idx] + beta_2 * xs[row_idx + 1];
            if (k < threshold) {
                if (k % 5 == 0) {
                    ys[k] = ys[k] * 1000 * row_idx;
                }
            }
        }

        std::transform(xs.begin(), xs.end(), xs_noise.begin(), xs.begin(), std::plus<float>());

        const size_t num_rounds = 5000;

        const DataSet train_data(nfeatures, xs, ys);

        const Booster fitter(seed);
        std::vector<float> pmf;
        std::vector<size_t> counts;
        for (size_t k = 0; k < num_rounds; ++k) {
            //std::cout << "========================" << std::endl;
            fitter.update_one(train_data, pmf, counts);
            for (size_t j = 0; j != pmf.size(); ++j) {
                //std::cout << std::fixed << std::setprecision(2);
                //std::cout << "P(x1 = " << std::setw(7) << std::left << xs[j * nfeatures] << ", x2 = ";
                //std::cout << std::setw(7) << std::left << xs[j * nfeatures + 1] << ") = ";
                //std::cout << std::setw(7) << std::left << pmf[j] << "count[x] = ";
                //std::cout << std::fixed << std::setprecision(0);
                //std::cout << std::setw(7) << std::left << counts[j] << "Y = " << ys[j] << std::endl;
            }
        }
        //std::cout << "Seed: " << seed << std::endl;

        //std::cout << std::max_element(counts.begin(), counts.end()) - counts.begin() << std::endl;

/*
        std::vector<float> yhats;
        tree.predict(xs, yhats);

        // split is done on feature 0, split value 3.4
        const float left_leaf = 5.25f;
        const float right_leaf = -18.1f;

        CPPUNIT_ASSERT_DOUBLES_EQUAL(left_leaf, yhats[0], m_tolerance);
        CPPUNIT_ASSERT_DOUBLES_EQUAL(left_leaf, yhats[1], m_tolerance);
        CPPUNIT_ASSERT_DOUBLES_EQUAL(right_leaf, yhats[2], m_tolerance);
        CPPUNIT_ASSERT_DOUBLES_EQUAL(right_leaf, yhats[3], m_tolerance);*/
    }

    void BoosterTest::test_predict_depth2() {
/*        const std::vector<float> xs {
            3.15f, 8.19f,
            5.11f, 3.10f,
            3.61f, 6.14f,
            6.77f, 4.32f,
            5.93f, 6.01f,
            5.65f, 4.63f,
            6.36f, 6.02f,
            5.20f, 3.72f };

        const std::vector<float> ys {
            18.49f,
            18.02f,
            17.53f,
            20.57f,
            20.93f,
            14.59f,
            23.49f,
            23.30f
        };
        const std::vector<float> new_xs {
            2.15f, 8.19f,
            5.40f, 3.10f,
            5.7f, 5.9f,
            6.77f, 8.12f,
        };

        // top-level split on feature 0 at value 5.65
        // LHS second level split on feature 0 at value 5.20
        // RHS second level split on feature 1 at value 6.01
        // lr = left, then right, etc.
        const float ll_leaf = 19.335f;
        const float lr_leaf = 14.59f;
        const float rl_leaf = 20.75f;
        const float rr_leaf = 23.49f;

        const std::vector<float> expected {
            ll_leaf,
            lr_leaf,
            rl_leaf,
            rr_leaf
        };

        const size_t nfeatures = 2;
        const size_t depth = 2;

        SequentialSampler sampler(0, ys.size());
        Partitioner builder(nfeatures, depth, xs, ys);
        builder.build(sampler);

        const RegressionTree tree(builder);

        std::vector<float> yhats;
        tree.predict(new_xs, yhats);

        CPPUNIT_ASSERT_DOUBLES_EQUAL(expected[0], yhats[0], m_tolerance);
        CPPUNIT_ASSERT_DOUBLES_EQUAL(expected[1], yhats[1], m_tolerance);
        CPPUNIT_ASSERT_DOUBLES_EQUAL(expected[2], yhats[2], m_tolerance);
        CPPUNIT_ASSERT_DOUBLES_EQUAL(expected[3], yhats[3], m_tolerance);*/
    }
}
